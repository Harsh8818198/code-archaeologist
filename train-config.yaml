model:
  model_name: "Qwen/Qwen2.5-1.5B-Instruct"
  trust_remote_code: true
  torch_dtype_str: "bfloat16"
  model_max_length: 2048

data:
  train:
    collator_name: "text_completions_only_with_padding"
    target_col: "messages"
    datasets:
      - dataset_name: "text_sft_jsonl"
        dataset_path: "/home/shank/projects/code-archaeologist/training-data.jsonl"
        split: "train"
        shuffle: true
        seed: 42

training:
  trainer_type: "TRL_SFT"
  output_dir: "./output/archaeologist-model"
  num_train_epochs: 3
  learning_rate: 2.0e-4
  use_peft: true

  # 6 GB VRAM safety
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 4
  enable_gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  ddp_find_unused_parameters: false

  logging_steps: 10
  save_steps: 0
  eval_strategy: "no"
  dataloader_num_workers: "auto"
  dataloader_prefetch_factor: 32
  include_performance_metrics: true
  log_model_summary: false

peft:
  lora_r: 64
  lora_alpha: 64
  lora_dropout: 0.1

  q_lora: true
  q_lora_bits: 4
  bnb_4bit_quant_type: "fp4"
  use_bnb_nested_quant: true
  bnb_4bit_compute_dtype: "bfloat16"

  peft_save_mode: "ADAPTER_ONLY"
